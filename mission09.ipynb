{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebef0520",
   "metadata": {},
   "source": [
    "# 미션9\n",
    "\n",
    "FashionMNIST 데이터셋의 각 패션 아이템(예: 티셔츠, 바지, 스니커즈 등)을 조건부로 생성하는 작업을 수행합니다.\n",
    "\n",
    "각 클래스에 해당하는 이미지를 생성하는 cGAN (Conditional GAN) 모델을 직접 설계하고 학습시켜 보세요.\n",
    "\n",
    "**데이터 구성**:\n",
    "- **훈련 데이터**: 60,000장의 이미지\n",
    "- **테스트 데이터**: 10,000장의 이미지\n",
    "- 28×28 크기의 흑백 이미지 **(10개 클래스)**\n",
    "\n",
    "**클래스 목록**:\n",
    "- T-shirt/top\n",
    "- Trouser\n",
    "- Pullover\n",
    "- Dress\n",
    "- Coat\n",
    "- Sandal\n",
    "- Shirt\n",
    "- Sneaker\n",
    "- Bag\n",
    "- Ankle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c7931",
   "metadata": {},
   "source": [
    "### 전체 흐름 요약\n",
    "\n",
    "1. 데이터 준비 – FashoinMNIST 데이터셋을 다운 받아 정규화(-1~1) 시키고 -> 라벨 One-Hot 또는 임베딩을 준비합니다.\n",
    "2. DataLoader 정의 - 배치(batch)size, 셔플, 멀티 스레드 로딩합니다.\n",
    "3. 모델 설계 – Generator - 조건부 입력 (노이즈+라벨) -> `convTranspose2d` 블록 + Conditional BatchNorm\n",
    "4. 모델 설계 – Discriminator - 조건부 입력(이미지 + 라벨) -> `Conv2d` 블록 + Projection Discriminator\n",
    "5. 손실·옵티마이저 정의 - BCELoss 또는 WGAN‑GP → Adam/AdamW\n",
    "6. 학습 루프 구현 - 에포크, 디스크리미네이터·제너레이터 순차 업데이트, 로깅, 체크포인트 저장\n",
    "7. 중간·최종 결과 시각화 - 클래스별 샘플 이미지 그리드, 학습 로스·시각화\n",
    "8. 정량적 평가 - FID(Fréchet Inception Distance), IS(Inception Score) 혹은 K‑NN 기반 평가\n",
    "9. 모델 저장·로드 & 인퍼런스 함수 - .pth 저장·로드, generate(class_id, n_samples)\n",
    "\n",
    "아래에서 각 단계별 핵심 코드를 한 줄씩 짚어 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d67a16",
   "metadata": {},
   "source": [
    "### 프로젝트 할 일 목록 (ToDO List)\n",
    "\n",
    "1. 환경 설정 및 데이터 로드: 필요한 라이브러리를 임포트하고 Fashion-MNIST 데이터셋을 불러옵니다.\n",
    "2. 데이터 전처리 및 시각화: 데이터를 정규화하고, 샘플 이미지를 확인하여 데이터셋의 구성을 이해합니다.\n",
    "3. cGAN 모델 설계:\n",
    "   * 생성자 (Generator): 노이즈 벡터와 조건부 레이블을 입력받아 이미지를 생성하는 모델을 설계합니다.\n",
    "   * 판별자 (Discriminator): 이미지와 조건부 레이블을 입력받아 해당 이미지가 진짜인지 가짜인지 판별하는 모델을\n",
    "         설계합니다.\n",
    "4. 학습 환경 설정: 모델, 손실 함수, 옵티마이저를 초기화하고 학습에 필요한 하이퍼파라미터를 정의합니다.\n",
    "5. cGAN 모델 학습: 생성자와 판별자를 번갈아 학습시키는 훈련 루프를 구현하고, 주기적으로 생성된 이미지를 저장하여 학습\n",
    "      과정을 모니터링합니다.\n",
    "6. 조건부 이미지 생성 및 평가: 학습된 모델을 사용하여 각 클래스별 이미지를 생성하고, 생성된 이미지의 품질을 시각적으로 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8b5bb",
   "metadata": {},
   "source": [
    "## 환경 설정 및 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69338c3",
   "metadata": {},
   "source": [
    "- Python 3.10.19+\n",
    "- PyTorch 2.5.1+ (CUDA 12 권장)\n",
    "- torchvision, tqdm, matplotlib, numpy\n",
    "- 정량적 평가용: torch-fidelity (FID/IS), scikit-learn (K‑NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af1e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e32117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04ae949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape: torch.Size([64, 1, 28, 28]), lbls.shape: torch.Size([64])\n",
      "idx_to_class : {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Dataset 및 DataLoader 설정\n",
    "# ============================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # [-1, 1] 범위로 정규화\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 배치 형태 확인\n",
    "imgs, lbls = next(iter(train_loader))\n",
    "print(f'imgs.shape: {imgs.shape}, lbls.shape: {lbls.shape}')\n",
    "# -> torch.Size([128, 1, 28, 28]) torch.Size([128])\n",
    "\n",
    "# 클래스 이름 (FashionMNIST 클래스)\n",
    "idx_to_class = {i: class_name for i, class_name in enumerate(train_dataset.classes)}\n",
    "print(f'idx_to_class : {idx_to_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7645a",
   "metadata": {},
   "source": [
    "## cGAN 모델 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c69174",
   "metadata": {},
   "source": [
    "조건부 GAN에서 **Generator**는 두 가지 입력을 받습니다.\n",
    "\n",
    "1. **노이즈 벡터** `z ∈ ℝ^{latent_dim}` (예: 100 차원, 표준 정규분포)\n",
    "2. **클래스 라벨** `y ∈ {0,…,9}` → 임베딩 → `y_emb ∈ ℝ^{embed_dim}`\n",
    "\n",
    "두 벡터를 **concatenation**하거나 **Conditional BatchNorm**을 통해 결합합니다.\n",
    "\n",
    "여기서는 **Concatenation + Linear** 방식과 **Conditional BatchNorm**(optional) 두 가지 구현을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0561886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 생성자 (Generator) 클래스\n",
    "# ============================================\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, latent_dim=100, embed_dim=50, img_shape=(1, 28, 28)):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.img_shape = img_shape\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.embed_dim = embed_dim\n",
    "\n",
    "#         # 라벨 임베딩 (10 → embed_dim)\n",
    "#         self.label_emb = nn.Embedding(10, embed_dim)\n",
    "\n",
    "#         # 입력 차원: latent_dim + embed_dim\n",
    "#         self.init_size = 7   # 7x7 feature map (28 / 4)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(latent_dim + embed_dim, 128 * self.init_size * self.init_size),\n",
    "#             nn.BatchNorm1d(128 * self.init_size * self.init_size),\n",
    "#             nn.LeakyReLU(0.2, inplace=True)\n",
    "#         )\n",
    "\n",
    "#         # Upsampling block\n",
    "#         self.deconv = nn.Sequential(\n",
    "#             # 128 x 7 x 7 -> 64 x 14 x 14\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "#             # 64 x 14 x 14 -> 1 x 28 x 28\n",
    "#             nn.ConvTranspose2d(64, img_shape[0], kernel_size=4, stride=2, padding=1),\n",
    "#             nn.Tanh()   # output in [-1, 1]\n",
    "#         )\n",
    "\n",
    "#     def forward(self, noise, labels):\n",
    "#         # embedding\n",
    "#         label_embedding = self.label_emb(labels)\n",
    "\n",
    "#         # concat noise + label\n",
    "#         gen_input = torch.cat((noise, label_embedding), dim=1)   # (batch, latent+embed)\n",
    "\n",
    "#         out = self.fc(gen_input)\n",
    "#         out = out.view(out.size(0), 128, self.init_size, self.init_size)   # (B,128,7,7)\n",
    "#         img = self.deconv(out)\n",
    "#         return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e574f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 생성자 (Generator) 클래스\n",
    "# ============================================\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, num_classes, img_size, channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.channels = channels\n",
    "        \n",
    "        # 레이블을 위한 임베딩 레이어\n",
    "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
    "        \n",
    "        # 노이즈와 임베딩된 레이블을 합친 벡터를 처리하는 모델\n",
    "        self.model = nn.Sequential(\n",
    "            # 입력 크기 : z_dim + num_classes\n",
    "            # 초기 이미지를 만들기 위해 7x7 크기의 256개의 채널로 변환\n",
    "            nn.Linear(z_dim + num_classes , 256 * 7* 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1,(256,7,7)), # (배치, 256, 7, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1,(256,7,7)), # (배치, 256, 7, 7)\n",
    "            \n",
    "            # 14x14 로 업샘플링\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False), #(배치, 128, 14, 14)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 28x28 업샘플링\n",
    "            nn.ConvTranspose2d(128,channels, kernel_size=4, stride=2, padding=1, bias=False), #(배치, channels, 28, 28)\n",
    "            \n",
    "            # 출력 범위를 -1 ~ 1 정규화\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, labels):\n",
    "        # 레이블 임베딩\n",
    "        c = self.label_embedding(labels)\n",
    "        # 노이즈와 레이블 결합\n",
    "        x = torch.cat([z,c],1)\n",
    "        # 이미지 생성\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c7b92",
   "metadata": {},
   "source": [
    "onditional BatchNorm\n",
    "\n",
    "- **조건부 배치 정규화(CBND)**: 라벨별 `γ, β` 파라미터를 학습하고, `nn.BatchNorm2d` 대신 `ConditionalBatchNorm2d` 구현.\n",
    "- 이 방법은 **Projection GAN**이나 **Self‑Attention GAN**에 자주 쓰이며, 이미지 품질을 약간 향상시킵니다.\n",
    "\n",
    "> **참고**: 여기서는 기본 Concatenation 방식만 구현하되, `ConditionalBatchNorm2d` 클래스를 별도 코드 블록에 넣어 `if use_cbn:` 형태로 스위치할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab4498",
   "metadata": {},
   "source": [
    "Discriminator도 라벨 정보를 **조건부**으로 활용합니다.\n",
    "\n",
    "**두 가지 방식** 중 하나를 선택:\n",
    "\n",
    "1. **Concat 방식**: 이미지와 라벨 임베딩을 채널 차원에 붙여서 `Conv2d` 로 학습.  \n",
    "2. **Projection 방식** : 라벨 임베딩을 **feature space**에 내적해 스칼라를 더함 → 논문 *\"Conditional Image Synthesis With Auxiliary Classifier GANs\"* 에서 영감을 받음.\n",
    "\n",
    "> **Tip** – `torch.nn.utils.spectral_norm` 를 `nn.Conv2d` 혹은 `nn.Linear` 에 적용하면 **스펙트럴 정규화**가 자동으로 적용돼 훈련 안정성이 크게 향상됩니다. 필요 시 `torch.nn.utils.spectral_norm` 로 래핑해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a57ff580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Projection Discriminator (Miyato et al., 2018)\n",
    "    f(x) = h(x) + <ψ(y), φ(x)>\n",
    "    where:\n",
    "        - h(x): scalar output from CNN on image\n",
    "        - ψ(y): label embedding (size = embed_dim)\n",
    "        - φ(x): image feature vector (size = embed_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=50, img_shape=(1, 28, 28)):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.label_emb = nn.Embedding(10, embed_dim)\n",
    "\n",
    "        # CNN backbone (downsample 28->14->7)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0], 64, kernel_size=4, stride=2, padding=1),   # 1x28x28 -> 64x14x14\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),            # 64x14x14 -> 128x7x7\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Final feature -> scalar (h(x))\n",
    "        self.adv_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 1)   # scalar real/fake logit\n",
    "        )\n",
    "\n",
    "        # Feature projection to embed_dim (φ(x))\n",
    "        self.proj = nn.Linear(128 * 7 * 7, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # img : (B,1,28,28), labels : (B,)\n",
    "        feats = self.conv(img)                 # (B,128,7,7)\n",
    "        feats_flat = feats.view(feats.size(0), -1)   # (B,128*7*7)\n",
    "\n",
    "        # scalar logit from image alone\n",
    "        out_adv = self.adv_head(feats_flat)    # (B,1)\n",
    "\n",
    "        # projection term <ψ(y), φ(x)>\n",
    "        phi = self.proj(feats_flat)            # (B, embed_dim)\n",
    "        psi = self.label_emb(labels)           # (B, embed_dim)\n",
    "\n",
    "        proj = torch.sum(phi * psi, dim=1, keepdim=True)   # (B,1)\n",
    "\n",
    "        # Final discriminator logit\n",
    "        out = out_adv + proj\n",
    "        return out.squeeze()   # (B,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e94142",
   "metadata": {},
   "source": [
    "## 손실·옵티마이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bac0c93",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2e-4\u001b[39m\n\u001b[1;32m      8\u001b[0m b1, b2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.999\u001b[39m   \u001b[38;5;66;03m# Adam 파라미터\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m optimizer_G \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mgenerator\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39m(b1, b2))\n\u001b[1;32m     10\u001b[0m optimizer_D \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(discriminator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39m(b1, b2))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 전통적인 BCE‑GAN\n",
    "# ============================================\n",
    "adversarial_loss = nn.BCEWithLogitsLoss()   # logits 입력 (torch.nn.functional.binary_cross_entropy_with_logits)\n",
    "\n",
    "# 옵티마이저\n",
    "lr = 2e-4\n",
    "b1, b2 = 0.5, 0.999   # Adam 파라미터\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dab6bd",
   "metadata": {},
   "source": [
    "> `BCELoss` 로 시작하고, 학습이 잘 안 될 경우 `WGAN‑GP` 로 전환을 권장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# WGAN‑GP 손실: D(real) - D(fake) + λ * GP\n",
    "# ============================================\n",
    "lambda_gp = 10.0\n",
    "\n",
    "def gradient_penalty(D, real, fake, labels):\n",
    "    # Random weight for interpolation between real and fake\n",
    "    alpha = torch.rand(real.size(0), 1, 1, 1, device=real.device)\n",
    "    interpolates = (alpha * real + ((1 - alpha) * fake)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates, labels)\n",
    "    fake = torch.ones(d_interpolates.size(), device=real.device)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04824a21",
   "metadata": {},
   "source": [
    "## 환경 설정 및 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5aabeb",
   "metadata": {},
   "source": [
    "> **Tip** – `log_interval` 를 `len(train_loader)//10` 정도로 잡으면 매 에포크마다 10번 정도 로스가 출력돼 학습 진행 상황을 빠르게 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89868156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BCE‑GAN\n",
    "# ============================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# 고정된 노이즈와 라벨(시각화용)\n",
    "fixed_noise = torch.randn(10, 100, device=device)   # 10개 클래스당 1개씩\n",
    "fixed_labels = torch.arange(10, device=device)      # 0~9\n",
    "\n",
    "def sample_and_save(epoch):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        gen_imgs = generator(fixed_noise, fixed_labels).cpu()\n",
    "        # denorm\n",
    "        gen_imgs = gen_imgs * 0.5 + 0.5\n",
    "        grid = torchvision.utils.make_grid(gen_imgs, nrow=5, normalize=False)\n",
    "        # 저장\n",
    "        out_path = f'output/epoch_{epoch:04d}.png'\n",
    "        torchvision.utils.save_image(grid, out_path)\n",
    "    generator.train()\n",
    "\n",
    "num_epochs = 100\n",
    "log_interval = 200   # step마다 로스 출력\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for i, (imgs, labels) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch}/{num_epochs}')):\n",
    "        batch_size = imgs.size(0)\n",
    "        real = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Discriminator\n",
    "        # -----------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        real_validity = discriminator(real, labels)\n",
    "        real_loss = F.binary_cross_entropy_with_logits(\n",
    "            real_validity, torch.ones_like(real_validity))\n",
    "\n",
    "        # Fake loss\n",
    "        z = torch.randn(batch_size, 100, device=device)\n",
    "        fake_imgs = generator(z, labels).detach()\n",
    "        fake_validity = discriminator(fake_imgs, labels)\n",
    "        fake_loss = F.binary_cross_entropy_with_logits(\n",
    "            fake_validity, torch.zeros_like(fake_validity))\n",
    "\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample new noise\n",
    "        z = torch.randn(batch_size, 100, device=device)\n",
    "        gen_imgs = generator(z, labels)\n",
    "        # Generator tries to fool the discriminator\n",
    "        validity = discriminator(gen_imgs, labels)\n",
    "        g_loss = F.binary_cross_entropy_with_logits(\n",
    "            validity, torch.ones_like(validity))\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ----- logging -----\n",
    "        if i % log_interval == 0:\n",
    "            tqdm.write(f\"[E{epoch}/{num_epochs} I{i}/{len(train_loader)}] \"\n",
    "                       f\"D loss: {d_loss.item():.4f} | G loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # ----- epoch-end tasks -----\n",
    "    # 1) 샘플 이미지 저장\n",
    "    sample_and_save(epoch)\n",
    "\n",
    "    # 2) 체크포인트 저장\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "    }, f'checkpoints/ckpt_epoch_{epoch:04d}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a6dca",
   "metadata": {},
   "source": [
    "WGAN‑GP 차이점\n",
    "- **Discriminator**를 `n_critic` (예: 5) 번 업데이트\n",
    "- **Loss**: `d_loss = -(real_validity.mean() - fake_validity.mean()) + λ * gradient_penalty`\n",
    "- **Generator** loss: `-fake_validity.mean()`\n",
    "\n",
    "코드 블록을 별도 셀에 넣고, `use_wgan_gp=True` 로 스위치하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea230fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# WGAN‑GP\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3a48e",
   "metadata": {},
   "source": [
    "### 중간·최종 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 샘플 시각화\n",
    "# ============================================\n",
    "\n",
    "def show_samples(dataset, n=10):\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n*1.2, 1.5))\n",
    "    for i in range(n):\n",
    "        img, label = dataset[i]\n",
    "        img = img.squeeze().numpy() * 0.5 + 0.5   # denorm for display\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(idx_to_class[label], fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_samples(train_set, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28171f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 클래스별 이미지 그리드\n",
    "# ============================================\n",
    "def visualize_classes(generator, n_per_class=8):\n",
    "    generator.eval()\n",
    "    all_imgs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for class_id in range(10):\n",
    "            # 동일 라벨에 대해 여러 노이즈 샘플링\n",
    "            z = torch.randn(n_per_class, 100, device=device)\n",
    "            labels = torch.full((n_per_class,), class_id, dtype=torch.long, device=device)\n",
    "            gen_imgs = generator(z, labels).cpu()\n",
    "            all_imgs.append(gen_imgs)\n",
    "            all_labels.extend([class_names[class_id]] * n_per_class)\n",
    "\n",
    "    # (10, n_per_class, 1, 28, 28) -> (10*n_per_class, 1, 28, 28)\n",
    "    grid_imgs = torch.cat(all_imgs, dim=0)\n",
    "    grid = torchvision.utils.make_grid(grid_imgs, nrow=n_per_class, normalize=True, pad_value=1)\n",
    "\n",
    "    plt.figure(figsize=(n_per_class, 10))\n",
    "    plt.title(\"Conditional Generation (rows: class, cols: samples)\")\n",
    "    plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    generator.train()\n",
    "\n",
    "visualize_classes(generator, n_per_class=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ad902",
   "metadata": {},
   "source": [
    "> **Tip** – `TensorBoard` 를 사용하려면 `torch.utils.tensorboard.SummaryWriter` 로 `add_scalar('Loss/D', d_loss, global_step)` 등을 기록하면, Colab에서도 `tensorboard --logdir=runs` 로 실시간 모니터링 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07826bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 로스 곡선 시각화\n",
    "# ============================================\n",
    "\n",
    "log_df = pd.read_csv('training_log.csv')   # epoch, step, d_loss, g_loss (CSV 저장해두었다면)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(log_df['step'], log_df['d_loss'], label='Discriminator')\n",
    "plt.plot(log_df['step'], log_df['g_loss'], label='Generator')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec63ca5",
   "metadata": {},
   "source": [
    "### 정량적 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a32a01",
   "metadata": {},
   "source": [
    "**Fashion‑MNIST** 은 흑백 28×28 라는 특성 때문에 일반적인 `Inception Score`(IS) 를 쓰기엔 적합도가 낮습니다. 대신 아래 두 지표를 활용할 수 있습니다.\n",
    "\n",
    "| 지표 | 설명 | 구현 방법 |\n",
    "|------|------|-----------|\n",
    "| **FID (Fréchet Inception Distance)** | 실제 이미지와 생성 이미지의 특징 분포 차이 (InceptionV3 활용) | `torch-fidelity` (`fid.compute_fid`) |\n",
    "| **K‑NN based Precision / Recall** | Feature space(K‑NN)에서 실제와 생성 이미지간의 정밀도·재현율 측정 | `scikit-learn` KNN, 또는 `torch_fidelity` 의 `kernel_inception_distance` 옵션 |\n",
    "| **Classifier Accuracy (조합)** | 사전학습된 (또는 직접 학습한) Fashion‑MNIST 분류기로 생성 이미지에 라벨을 예측 → **조건부 정확도** 측정 | 직접 `CNN` 학습 후 `accuracy` 계산 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9e320",
   "metadata": {},
   "source": [
    "> **주의** – `torch-fidelity` 내부에서 `InceptionV3` 를 사용하므로, 28×28 이미지를 **Resize(299,299)** 로 자동 변환합니다. 이 과정이 오래 걸릴 수 있으니 GPU 사용을 권장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FID 계산\n",
    "# ============================================\n",
    "# 1) 실제 이미지 디렉터리와 생성 이미지 디렉터리를 준비\n",
    "#    (예: real/와 fake/ 폴더에 10k PNG 저장)\n",
    "\n",
    "# 실제 이미지 저장\n",
    "real_dir = 'fid/real/'\n",
    "fake_dir = 'fid/fake/'\n",
    "\n",
    "os.makedirs(real_dir, exist_ok=True)\n",
    "os.makedirs(fake_dir, exist_ok=True)\n",
    "\n",
    "# 실제 이미지 (train_set) 저장\n",
    "for idx, (img, _) in enumerate(train_set):\n",
    "    if idx >= 10000: break\n",
    "    torchvision.utils.save_image(\n",
    "        img * 0.5 + 0.5,  # denorm\n",
    "        os.path.join(real_dir, f'{idx:05d}.png')\n",
    "    )\n",
    "\n",
    "# 생성 이미지 저장 (클래스 별 1k씩)\n",
    "for class_id in range(10):\n",
    "    for i in range(1000):\n",
    "        z = torch.randn(1, 100, device=device)\n",
    "        label = torch.tensor([class_id], device=device)\n",
    "        gen = generator(z, label).cpu()\n",
    "        torchvision.utils.save_image(\n",
    "            gen * 0.5 + 0.5,\n",
    "            os.path.join(fake_dir, f'class{class_id}_{i:05d}.png')\n",
    "        )\n",
    "\n",
    "# 2) FID 계산 (torch-fidelity)\n",
    "from torch_fidelity import calculate_metrics\n",
    "\n",
    "metrics = calculate_metrics(\n",
    "    input1=real_dir,\n",
    "    input2=fake_dir,\n",
    "    cuda=torch.cuda.is_available(),\n",
    "    verbose=True,\n",
    "    isc=False, # IS는 비활성화 (Inception 필요)\n",
    "    fid=True,\n",
    "    kid=False\n",
    ")\n",
    "\n",
    "print(f\"FID: {metrics['frechet_inception_distance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bda83",
   "metadata": {},
   "source": [
    "클래스‑조건부 정확도 (Conditional Consistency)\n",
    "\n",
    "1. **분류기 사전 학습** – `torchvision.models.resnet18` 을 `FashionMNIST` 로 fine‑tune(1채널 → 3채널 변환 필요).\n",
    "2. **생성 이미지에 라벨 예측** – `pred = classifier(gen_img)`.\n",
    "3. **정확도** `accuracy = (pred.argmax(1) == label).float().mean()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5eebd",
   "metadata": {},
   "source": [
    "> **Interpretation** – 10% 가 무작위 베이스라면, **50% 이상**이면 생성 이미지가 라벨 정보를 잘 반영하고 있다는 의미입니다. (예: 70%~80% 정도면 꽤 좋은 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7603e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 클래스‑조건부 정확도 계산\n",
    "# ============================================\n",
    "# 예시: 간단한 CNN classifier\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "# 학습 후 저장된 모델 로드\n",
    "classifier = SimpleCNN().to(device)\n",
    "classifier.load_state_dict(torch.load('classifier.pth'))\n",
    "\n",
    "# 조건부 정확도 측정\n",
    "def conditional_accuracy(generator, classifier, n_samples=2000):\n",
    "    generator.eval()\n",
    "    classifier.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples // 100):\n",
    "            batch = 100\n",
    "            z = torch.randn(batch, 100, device=device)\n",
    "            labels = torch.randint(0, 10, (batch,), device=device)\n",
    "            gen_imgs = generator(z, labels)\n",
    "            logits = classifier(gen_imgs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += batch\n",
    "    return correct / total\n",
    "\n",
    "print(f\"Conditional classification accuracy: {conditional_accuracy(generator, classifier)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e1140",
   "metadata": {},
   "source": [
    "### 모델 저장·로드 & 인퍼런스 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392c2870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 모델 저장\n",
    "# ============================================\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
    "\n",
    "# 로드 함수\n",
    "def load_generator(checkpoint_path='generator.pth', device='cpu'):\n",
    "    model = Generator().to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# 인퍼런스 함수\n",
    "def generate(class_id, n_samples=16, seed=None, device='cpu'):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    gen = load_generator(device=device)\n",
    "    z = torch.randn(n_samples, 100, device=device)\n",
    "    labels = torch.full((n_samples,), class_id, dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        imgs = gen(z, labels)\n",
    "    # denorm\n",
    "    imgs = imgs * 0.5 + 0.5\n",
    "    return imgs.cpu()\n",
    "\n",
    "# 예시 사용\n",
    "samples = generate(class_id=7, n_samples=9, seed=42)  # 스니커즈\n",
    "grid = torchvision.utils.make_grid(samples, nrow=3)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.title(\"Generated Sneakers\")\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
